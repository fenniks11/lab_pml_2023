{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron (MLP) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisi\n",
    "Merupakan salah satu jenis arsitektur jaringan saraf tiruan (JST)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron(MLP)sering digunakan untuk  berbagai aplikasi seperti:  \n",
    "- Pengenalan gambar(Image Recognition),\n",
    "- Identifikasi pembicara(Speaker Identification), \n",
    "- *Task* klasifikasi, contohnya Klasifikasi biji kopi, \n",
    "- Deteksi kecurangan(Fraud Detection), dan \n",
    "- Penandaan teks pendek(tagging short text).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arsitektur MLP\n",
    "MLP terdiri dari beberapa lapisan node, termasuk lapisan input, satu atau lebih lapisan tersembunyi(*hidden layer*), dan lapisan output.  \n",
    "1. **Input Layer**: Ini adalah lapisan pertama dari MLP. Setiap node dalam lapisan ini mewakili fitur atau atribut input dari data Anda. Jumlah node dalam lapisan ini sesuai dengan jumlah fitur yang digunakan dalam masalah Anda.\n",
    "\n",
    "2. **Hidden Layers**: Setelah lapisan input, Anda dapat memiliki satu atau lebih lapisan tersembunyi (hidden layers). Setiap lapisan tersembunyi terdiri dari banyak node (neuron) yang saling terhubung. Jumlah dan ukuran lapisan tersembunyi ini dapat bervariasi tergantung pada kompleksitas masalah. Lapisan tersembunyi ini digunakan untuk mengekstraksi pola-pola yang lebih kompleks dari data.\n",
    "\n",
    "3. **Output Layer**: Ini adalah lapisan terakhir dari MLP. Jumlah node dalam lapisan ini sesuai dengan jenis output yang ingin Anda hasilkan. Misalnya, jika Anda melakukan klasifikasi, jumlah node di lapisan output akan sesuai dengan jumlah kelas yang berbeda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://i.ibb.co/H23k8Vc/MLP-arsitektur.webp\" alt=\"MLP-arsitektur\" border=\"0\">\n",
    "<figcaption>Gbr. Arsitektur dari sebuah multilayer perceptron dengan dua input.</figcaption>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP dilatih menggunakan **algoritma backpropagation**, yang menyesuaikan bobot koneksi antara node untuk meminimalkan kesalahan antara output yang diprediksi dan output aktual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogi Sederhana\n",
    "Seperti sebuah tumpukan buku dimana tumpukan itu adalah lapisan, dengan **buku teratas sebagai lapisan input, buku tengah sebagai lapisan tersembunyi, dan buku terbawah sebagai lapisan output.**\n",
    "\n",
    "Setiap halaman dalam buku berisi informasi yang diteruskan ke halaman berikutnya, dan setiap halaman memiliki catatan kecil yang menentukan seberapa penting informasi itu. Saat membaca buku atau dalam hal ini menjalankan jaringan, akan ada pemahaman dari kombinasi informasi disetiap halaman untuk sampai pada kesimpulan akhir yang ada di halaman terakhir.\n",
    "\n",
    "Catatan-catatan kecil dalam buku juga dapat diubah-ubah, dalam hal ini digambarkan sebagai update bobot, tujuannya agar hasil akhir lebih mendekati yang seharusnya. Begitulah cara MLP bekerja, dengan mengatur bobot (catatan-catatan kecil) di setiap hubungan antara lapisan untuk meminimalkan kesalahan dan mendapatkan hasil yang lebih baik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP: Fungsi Aktivasi\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/r58YSdj/fungsi-aktivasi.webp\" alt=\"fungsi-aktivasi\" border=\"0\">\n",
    "<figcaption>Gbr.Berbagai jenis fungsi aktivasi</figcaption>\n",
    "</center>  \n",
    "\n",
    "Setiap node dalam MLP biasanya memiliki fungsi aktivasi. Fungsi ini memutuskan apakah node tersebut harus \"aktif\" atau tidak berdasarkan masukan yang diterimanya. Beberapa fungsi aktivasi umum adalah   \n",
    "- Leaky ReLU (Rectified Linear Unit)  \n",
    "LeakyReLU merupakan fungsi aktivasi perluasan dari fungsi aktivasi ReLU. Perbedaan dari ReLU dan LeakyReLU adalah dalam model pemetaan nilai inputnya. Jika ReLU akan memetaan dalam rentang 0 hingga x, tetapi jika LeakyReLU tidak terbatas (y=x*0,01).\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/1qwpkRt/Re-LUvs-Leaky-Re-LU.webp\" alt=\"Re-LUvs-Leaky-Re-LU\" border=\"0\">\n",
    "<figcaption>Gbr.ReLU vs Leaky ReLU</figcaption>\n",
    "</center>  \n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/CBrV8LY/parametric-Re-LU.webp\" alt=\"parametric-Re-LU\" border=\"0\">\n",
    "</center>\n",
    "- Sigmoid (Logistic Function), dan \n",
    "Nama lainnya adalah sigmoid biner atau logistic function, karena fungsi ini memetakan nilai ke dalam rentang 0â€“1.\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/cCdVbXH/sigmoid.webp\" alt=\"sigmoid\" border=\"0\">\n",
    "<figcaption>Gbr.Sigmoid function</figcaption>\n",
    "</center>  \n",
    "\n",
    "- Tangen Hiperbolik (Tanh)\n",
    "TanH atau Hyperbolic Tangent dan bisa disebut sigmoid bipolar. Fungsi ini mirip dengan fungsi sigmoid, hanya saja fungsi ini memetakan nilai input ke dalam rentang -1 hingga 1.\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/FDvmVLG/persamaan-tanh.webp\" alt=\"persamaan-tanh\" border=\"0\">\n",
    "<figcaption>Gbr.Persamaan tanh function</figcaption>\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/fxMkgfP/grafik-sigmoid-tanh.webp\" alt=\"grafik-sigmoid-tanh\" border=\"0\">\n",
    "<figcaption>Gbr.Grafik Sigmoid-tanh</figcaption>\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source code fungsi aktivasi  \n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "def tanh(x):\n",
    "#   return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) atau\n",
    "    return 2*sigmoid(2*x)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU\n",
    "def leakyRelu(x):\n",
    "    return np.where(x > 0, x, x * 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hal-hal umum yang harus diketahui dalam MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Algoritma Backpropagation**  \n",
    "Ini adalah algoritma yang digunakan untuk melatih MLP. Backpropagation menghitung gradien (derivative) dari fungsi kerugian (loss function) terhadap bobot-bobot dalam jaringan, dan kemudian mengguncang (update) bobot-bobot tersebut untuk mengurangi kesalahan prediksi. Selain itu, ada beberapa algoritma yang juga dipakai dalam MLP:\n",
    "Stochastic Gradient Descent (SGD), Momentum, AdaGrad, RMSprop, Adam, dan lain-lain.\n",
    "\n",
    "- **Loss Function (Fungsi Kerugian)**  \n",
    "Merupakan metrik yang digunakan untuk mengukur seberapa baik atau buruk kinerja jaringan pada tugas tertentu. Contoh fungsi kerugian termasuk Mean Squared Error (MSE) untuk regresi dan Cross-Entropy Loss untuk klasifikasi.\n",
    "\n",
    "- **Optimizers**  \n",
    "Ada berbagai algoritma optimasi yang digunakan dalam pelatihan jaringan saraf, seperti Stochastic Gradient Descent (SGD), Adam, dan RMSprop. Optimizer mengatur bagaimana bobot dalam jaringan diperbarui selama pelatihan.\n",
    "\n",
    "- **Regularisasi**: Ini adalah teknik yang digunakan untuk mencegah overfitting dalam jaringan saraf. L1 dan L2 regularization, serta dropout, adalah contoh teknik regulasi yang umum digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soal\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/ZfpfYRq/arsitektur-jaringan-MLP.png\" alt=\"arsitektur-jaringan-MLP\" border=\"0\">\n",
    "<figcaption>Gbr.Arsitektur Jaringan MLP</figcaption>\n",
    "</center>\n",
    "Gambar di atas adalah jaringan saraf dengan dua input, dua neuron tersembunyi, dan dua neuron output. Selain itu, neuron tersembunyi dan output akan mencakup bias.\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/K91Y9Vg/soal-MLP.png\" alt=\"soal-MLP\" border=\"0\">\n",
    "<figcaption>Gbr.Soal MLP</figcaption>\n",
    "</center>  \n",
    "\n",
    "Tujuan dari backpropagation adalah untuk **mengoptimalkan bobot-bobot** sehingga jaringan saraf dapat belajar bagaimana memetakan input yang sembarang menjadi output yang benar.\n",
    "\n",
    "Dari gambar di atas, terdapat satu set data pelatihan saja: dengan input 0,05 dan 0,10, yang akan dipetakan agar jaringan saraf menghasilkan output 0,01 dan 0,99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah-Langkah Penyelesaian\n",
    "**1. Forward Pass**\n",
    "Menghitung total input ke setiap neuron di lapisan tersembunyi, kemudian menyusutkan total input tersebut menggunakan fungsi aktivasi (fungsi logistik/sigmoid), proses ini diulangi terus menerus hingga neuron lapisan output.  \n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/jrWcpJW/rumus-hitung-net-backpropagation.png\" alt=\"rumus-hitung-net-backpropagation\" border=\"0\">\n",
    "<figcaption>Gbr.Rumus perhitungan net</figcaption>\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://i.ibb.co/Kh8t3zz/cara-penggunaan-rumus-hitung-net-backpropagation.png\" alt=\"cara-penggunaan-rumus-hitung-net-backpropagation\" border=\"0\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/hDGshsN/outh1.png\" alt=\"outh1\" border=\"0\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/8N76Pm5/neth2.png\" alt=\"neth2\" border=\"0\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://i.ibb.co/8P8xbYg/neto1.png\" alt=\"neto1\" border=\"0\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/mbgSYWB/hasil-neto1.png\" alt=\"hasil-neto1\" border=\"0\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/6t7HR9b/outo1.png\" alt=\"outo1\" border=\"0\">\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/svJ6rLn/outo2.png\" alt=\"outo2\" border=\"0\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Hitung Error**\n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/txCR71W/rumus-error.png\" alt=\"rumus-error\" border=\"0\">\n",
    "<figcaption>Gbr.Rumus perhitungan error</figcaption>\n",
    "<img src=\"https://i.ibb.co/H416jH8/penggunaan-rumus-error.png\" alt=\"penggunaan-rumus-error\" border=\"0\">  \n",
    "</center>  \n",
    "<center>\n",
    "<img src=\"https://i.ibb.co/vxPg5Jk/error2.png\" alt=\"error2\" border=\"0\">\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<figcaption>Jumlahkan error</figcaption>\n",
    "<img src=\"https://i.ibb.co/rmzHzD7/jumlah-error.png\" alt=\"jumlah-error\" border=\"0\">\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Backward Pass**  \n",
    "Pertemuan selanjutnya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
